"""
LLM ä¼˜åŒ–æ¨¡å—
ä½¿ç”¨ LiteLLM è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ– markdown å†…å®¹
"""

from typing import Optional
import typer
from litellm import completion
import os


def optimize_markdown_with_llm(
    content: str,
    api_key: Optional[str] = None,
    api_base: Optional[str] = None,
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.3,
) -> Optional[str]:
    """
    ä½¿ç”¨ LLM ä¼˜åŒ– markdown å†…å®¹
    æ¸…ç†å¹¿å‘Šå’Œå¤šä½™æ–‡æ¡ˆ

    å‚æ•°:
        content: åŸå§‹ markdown å†…å®¹
        api_key: APIå¯†é’¥
        api_base: APIåŸºç¡€URL
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°

    è¿”å›:
        ä¼˜åŒ–åçš„ markdown å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    try:
        typer.echo("ğŸ¤– æ­£åœ¨ä½¿ç”¨ LLM ä¼˜åŒ–å†…å®¹...")

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæä¾›ï¼‰
        if api_key:
            # æ£€æµ‹providerå¹¶è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡
            provider = _detect_provider(model)
            if provider:
                env_key = f"{provider.upper()}_API_KEY"
                os.environ[env_key] = api_key

        # æ„å»ºè¯·æ±‚å‚æ•°
        kwargs = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Markdown å†…å®¹ç¼–è¾‘åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. æ¸…ç†æ–‡æ¡£ä¸­çš„å¹¿å‘Šå†…å®¹
2. åˆ é™¤æ— å…³çš„è¥é”€ä¿¡æ¯
3. åˆ é™¤é¡µé¢å¯¼èˆªã€ä¾§è¾¹æ ã€é¡µè„šç­‰éæ­£æ–‡å†…å®¹
4. ä¿ç•™æ–‡ç« çš„æ ¸å¿ƒå†…å®¹å’Œç»“æ„
5. ä¿æŒ Markdown æ ¼å¼çš„æ­£ç¡®æ€§å’Œå¯è¯»æ€§
6. ä¿®æ­£æ˜æ˜¾çš„æ ¼å¼é”™è¯¯
7. ä¸è¦æ·»åŠ ä»»ä½•è‡ªå·±çš„è¯„è®ºæˆ–è¯´æ˜ï¼Œç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ Markdown å†…å®¹""",
                },
                {
                    "role": "user",
                    "content": f"è¯·ä¼˜åŒ–ä»¥ä¸‹ Markdown å†…å®¹ï¼Œå»é™¤å¹¿å‘Šå’Œå¤šä½™æ–‡æ¡ˆï¼Œåªä¿ç•™æ ¸å¿ƒå†…å®¹ï¼š\n\n{content}",
                },
            ],
            "temperature": temperature,
            "drop_params": True,
        }

        # å¦‚æœæä¾›äº† api_baseï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if api_base:
            kwargs["api_base"] = api_base

        # è°ƒç”¨ LLM
        response = completion(**kwargs)

        # æå–ä¼˜åŒ–åçš„å†…å®¹
        optimized_content = response.choices[0].message.content

        if not optimized_content or len(optimized_content.strip()) == 0:
            typer.echo("âš ï¸  LLM è¿”å›çš„å†…å®¹ä¸ºç©º", err=True)
            return None

        typer.echo(f"âœ… ä¼˜åŒ–å®Œæˆï¼Œæ–°å†…å®¹é•¿åº¦: {len(optimized_content)} å­—ç¬¦")
        return optimized_content

    except Exception as e:
        typer.echo(f"âŒ LLM ä¼˜åŒ–å¤±è´¥: {e}", err=True)
        typer.echo("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ API é…ç½®æ˜¯å¦æ­£ç¡®", err=True)
        return None


def _detect_provider(model: str) -> Optional[str]:
    """
    æ£€æµ‹æ¨¡å‹æ‰€å±çš„ provider

    å‚æ•°:
        model: æ¨¡å‹åç§°

    è¿”å›:
        provider åç§°ï¼Œå¦‚æœæ— æ³•æ£€æµ‹è¿”å› None
    """
    # å¸¸è§çš„ provider å‰ç¼€
    provider_prefixes = {
        "gpt": "openai",
        "claude": "anthropic",
        "gemini": "gemini",
        "command": "cohere",
        "mistral": "mistral",
        "together": "together_ai",
        "ollama": "ollama",
        "azure": "azure",
        "bedrock": "bedrock",
        "vertex": "vertex_ai",
    }

    model_lower = model.lower()
    for prefix, provider in provider_prefixes.items():
        if model_lower.startswith(prefix):
            return provider

    # å¦‚æœæ¨¡å‹åç§°åŒ…å« '/' å¯èƒ½æ˜¯ together_ai æˆ– huggingface æ ¼å¼
    if "/" in model:
        return None  # è®© litellm è‡ªåŠ¨æ£€æµ‹

    return None
